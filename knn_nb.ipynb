{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An analysis of the State of the Union speeches - k nearest neighbors\n",
    "\n",
    "In this notebook, you should explore one question or idea of your own from this dataset.  Provide the code and computations here, and summarize your points in the [main](main.ipynb) notebook.\n",
    "\n",
    "# K-Nearest Neighbors \n",
    "\n",
    "In our previous notebooks, we've turned each speech in the dataset into a vector in $\\mathbb{R}^{n}$ where $n$ is the number of words across all of the speeches. Each speech vector $\\vec{v} \\in \\mathbb{R}^{n}$ represents a *count* of the number of occurrences of a particular word in that speech. For example, if George Washington's first state of the union used the word \"America\" 12 times, then the corresponding entry in that speech's vector would have a value of $12$. \n",
    "\n",
    "We want to use this data representation to train a classification algorithm known as **k-nearest neighbors**. We use a training set consisting of some of the Presidential speeches to train the algorithm. The vectors correspond to the data points, which the President who delivered the speech is the class label (so we have 45 class labels in total). Now, the algorithm has a set of \"speeches\" in $\\mathbb{R}^{n}$, each of which is labeled with a speaker. \n",
    "\n",
    "When predicting the speaker of a speech outside the training dataset, the algorithm looks the $k$ (say, 4$) \"nearest\" neighbors in this high-dimensional space and takes a vote amongst their labels. For example, the 4 nearest neighbors of a test data point might have labels George Washington, George Washington, George Washington, and Thomas Jefferson. The algorithm takes a plurality vote of the k-nearest neighbors, which in this case indicates that George Washington is most likely the person who delivered the speech. \n",
    "\n",
    "## Algorithmic Choices \n",
    "\n",
    "The k-nearest neighbors algorithm requires an integer choice of $k$, a notion of \"distance\" in the space being used (in our case, $\\mathbb{R}^{n}$), and a way of weighting the labels of the nearest neighbors. While we explore different values of $k$ in \"Grid Search\" below, we fix the weighting method and distance metric in our analysis. \n",
    "\n",
    "1. **Inverse-distance weighting**: A naive approach to k-nearest neighbors is to look at the class labels of the k-nearest neighbors and then take a simple vote of these labels. Inverse distance weighting gives more preference to \"closer\" neighbors, since these are more \n",
    "\n",
    "2. **Choice of distance metric**: Typically one might use the $\\ell_{2}$ norm $\\mathbb{R}^{n}$ as a measure of distance. But since our word-vectors measure a discrete probability distribution over $n$ words, we felt that the Jensen-Shannon Distance made more sense as a measure of difference between speeches. \n",
    "\n",
    "## Pre-Processing\n",
    "\n",
    "Here, we access the previous vectorized speech data, and pre-process by: \n",
    "\n",
    "1. Scaling the data so that the entries in each vector add up to $100$. This is necessary in a k-nearest neighbors algorithm because otherwise the distance from one vector to another is dominated by the components which happen to take on a larger range of values. \n",
    "\n",
    "2. Separating the data into a test and training set. The test set consists of 1 speech for each president, while the training set is all other speeches. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.neighbors.classification.KNeighborsClassifier"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "#load the data \n",
    "\n",
    "#scale the data \n",
    "scaler = MinMaxScaler(feature_range=(0, 100))\n",
    "\n",
    "#separate into a training and test set, with labels and vector values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the best k\n",
    "\n",
    "The number of neighbors to consider in k-nearest neighbors (that is, the value of $k$) is an important hyperparameter for which different choices might yield varying levels of accuracy. We do a search over $k \\in \\{1, 2, 3, 4, 5, 6, 7, 8\\}$ to decide which $k$ best minimizes the test error. The error measure is simply the number of speeches that are correctly classified. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from scipy.stats import entropy\n",
    "\n",
    "'''We will use J-S divergence as the metric in K-Nearest Neighbors'''\n",
    "def JSdiv(p, q):\n",
    "    \"\"\"Jensen-Shannon divergence.\n",
    "    \n",
    "    Compute the J-S divergence between two discrete probability distributions.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    p, q : array\n",
    "        Both p and q should be one-dimensional arrays that can be interpreted as discrete\n",
    "        probability distributions (i.e. sum(p) == 1; this condition is not checked).\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The J-S divergence, computed using the scipy entropy function (with base 2) for\n",
    "        the Kullback-Leibler divergence.\n",
    "    \"\"\"\n",
    "    m = (p + q) / 2\n",
    "    return (entropy(p, m, base=2.0) + entropy(q, m, base=2.0)) / 2\n",
    "\n",
    "'''Returns the percentage of the predicted labels which match the actual labels, as \n",
    "well as a list of the misclassified labels.'''\n",
    "def accuracy_rate(predicted_labels, actual_labels): \n",
    "    assert len(predicted_labels) == len(actual_labels), 'Different number of predictions than actual cases.'\n",
    "    \n",
    "    num_labels = len(predicted_labels)\n",
    "    misclassified_presidents = []\n",
    "    \n",
    "    for prediction, actual in zip(predicted_labels, actual_labels):\n",
    "        if prediction != actual: \n",
    "            misclassified_presidents.append(actual)\n",
    "    accuracy_rate = len(misclassified_presidents)/num_labels\n",
    "    return accuracy_rate, misclassified_presidents\n",
    "\n",
    "'''Trains a k-nearest neighbor classifier using J-S divergence as a metric and weighting by \n",
    "inverse distance. Returns the rate of correct speaker classification - that is, the rate at which\n",
    "the predicted speaker of a speech in the test dataset is the actual speaker.'''\n",
    "def knn_train_and_predict(k, train_speeches, train_labels, test_speeches, test_labels): \n",
    "    #instantiate an instance of the model \n",
    "    model = KNeighborsClassifier(n_neighbors=k, weights='distance', metric=JSdiv)\n",
    "    \n",
    "    #fit the model on the training data   \n",
    "    model.fit(train_speeeches, train_labels)\n",
    "    \n",
    "    #predict on the test data\n",
    "    prediction = model.predict(test_speeches)\n",
    "    \n",
    "    #compute accuracy rate \n",
    "    test_accuracy, misclassified_presidents = accuracy_rate(prediction, test_labels)\n",
    "    return test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for k=1 was 0.0\n",
      "Accuracy for k=2 was 0.0\n",
      "Accuracy for k=3 was 0.0\n",
      "Accuracy for k=4 was 0.0\n",
      "Accuracy for k=5 was 0.0\n"
     ]
    }
   ],
   "source": [
    "for k in range(1, 9): \n",
    "    #     test_accuracy = knn_train_and_predict(k) TODO\n",
    "    test_accuracy = 0.0\n",
    "    print('Accuracy for k={} was {}'.format(k, test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
