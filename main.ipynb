{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An analysis of the State of the Union speeches\n",
    "\n",
    "**Authors:** Yakub Akhmerov, Akhil Jalan, and Ye Zhong"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "\n",
    ".. brief summary of your conclusions here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ...\n",
    "\n",
    "Provide here a summary of the results of all 5 parts of the analysis, rendering the necessary figures, tables and conclusions.  You can load variables from your `results/` directory if you want to render summary statsitics or tables, and read figures from the `fig/` directory to display them, but there should be *no substantial computation* of any kind here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Notebook 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The raw data of all the speeches was formatted in the following way: There was metada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a list of all the unique words accross all speeches, we converted all of them into a list and then into a set, obviously to remove and redudancies. The length of that was taken to see the number of all unique words accross all speeches which ended up being ~19,000. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function word_vector was then created which took as parameters, a document and vocabulary. Word_vector's purpose was to return a vector for the inputted document along with the corresponding vocabulary. The algorithm to make it happen was to initialize an empty dictionary and to populate it with the words of the document as keys and the count as values. We iterated through the list of words, stored them as keys, and updated the dictionary to have the count of the word (by converting the document to a list and using the count() attribute). The keys were extracted and made into a list and returned to get the count of the words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A word matrix was then made ___ (Akhil fill out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate sparsity, a function, sparsity_calculator, was initialized which simply summed up the zeros and divided by the total entries in teh word matrix. We ended up with about ~93% sparsity which shows how many words are used. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook 4 dealt with Mutlidimensional Scaling and all that associated with it. The Word Matrix created from Notebook 3 was used. We wanted to create distance matrices between the words usages between presidents' speeches. This created a metric to show the differences between the presidents's words usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Findings from MDS "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
