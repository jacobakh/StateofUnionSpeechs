{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An analysis of the State of the Union speeches\n",
    "\n",
    "**Authors:** Yakub Akhmerov, Akhil Jalan, and Ken Zhong"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "\n",
    "In this project, we sought to explore a dataset of Presidential speeches ranging from 1790 to 2017. There are 227 unique speeches delivered by 42 presidents, which mostly consist of the annual State of the Union Address. By building a \"shared vocabulary\" consisting of all of the words used in at least one speech, we compared and contrasted Presidents by the frequency with which they used certain words. We mainly used dimensionality reduction techniques for visualization, and built a k-nearest neighbors classifier for prediction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ...\n",
    "\n",
    "Provide here a summary of the results of all 5 parts of the analysis, rendering the necessary figures, tables and conclusions.  You can load variables from your `results/` directory if you want to render summary statsitics or tables, and read figures from the `fig/` directory to display them, but there should be *no substantial computation* of any kind here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "The raw data came in the form of a text file. There was information in the beginnining, stating the president and the date of the given speech. The rest of the document had the speeches themselves, in order. \n",
    "\n",
    "In notebook 1, we loaded some simple meta-data (namely the President who gave the speech, the date it was given, and the title of the speech) into a Pandas dataframe. We noted that the majority of speeches in the dataset were delivered in December. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, we noticed a deficiency in the dataset - we lack speeches from Grover Cleveland's second administration, which was from 1893 to 1897. While not a severe impediment, it should be noted that analysis about Grover Cleveland in particular is distored."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Historical Trends \n",
    "\n",
    "In notebook 2, we explored some simple historical trends in speeches, such as number of sentences in a speech. \n",
    "\n",
    "TODO INCLUDE FIGURE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also grouped these trends by President. \n",
    "\n",
    "TODO INCLUDE FIGURE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Speech Vectorization \n",
    "\n",
    "After building a common vocabulary of words, in Notebook 3 vectorized the speeches. We represented each speech as a vector whose entries corresponded to the frequencies of those words. \n",
    "\n",
    "For example, suppose the entire stemmed vocabulary is ['dog', 'cat', 'bounce'] and the speech in question is 'bouncer bounce cats cat bounce.' Then the vector corresponding to this speech is $\\begin{bmatrix} 0 \\\\ 2 \\\\ 3 \\end{bmatrix}$. \n",
    "\n",
    "Given that there are 227 speeches and $n$ unique vocabulary words, we end up with vectors $v_1, ..., v_{227} \\in \\mathbb{R}^{n}$ corresponding to each speech. \n",
    "\n",
    "This vector representation of the speeches opens the door to a variety of analysis methods, all based in a vectorized notion of data. We discuss those analysis methods below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "The initial analysis we did was multidimensional scaling, with two seperate metrics, the euclidean distance, and the Jansen-Shannon distance. The idea was to incorporate the data into probability densities and compare the distances between them. This helps create a way of seperating them and seeing which ones relate to each other in whatever which way.\n",
    "\n",
    "The next analysis that was of our own choosing was to use K-nearest neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Notebook 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The raw data of all the speeches was formatted in the following way: There was metadata in the beginning of the speech, where the president speeches where written in the beginning and the date of them as well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a list of all the unique words accross all speeches, we converted all of them into a list and then into a set, obviously to remove and redudancies. The length of that was taken to see the number of all unique words accross all speeches which ended up being ~19,000. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function word_vector was then created which took as parameters, a document and vocabulary. Word_vector's purpose was to return a vector for the inputted document along with the corresponding vocabulary. The algorithm to make it happen was to initialize an empty dictionary and to populate it with the words of the document as keys and the count as values. We iterated through the list of words, stored them as keys, and updated the dictionary to have the count of the word (by converting the document to a list and using the count() attribute). The keys were extracted and made into a list and returned to get the count of the words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A word matrix was then made ___ (Akhil fill out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate sparsity, a function, sparsity_calculator, was initialized which simply summed up the zeros and divided by the total entries in teh word matrix. We ended up with about ~93% sparsity which shows how many words are used. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook 4 dealt with Mutlidimensional Scaling and all that associated with it. The Word Matrix created from Notebook 3 was used. We wanted to create distance matrices between the words usages between presidents' speeches. This created a metric to show the differences between the presidents's words usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Findings from MDS "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
