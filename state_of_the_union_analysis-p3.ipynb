{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# An analysis of the State of the Union speeches - Part 3\n",
    "# Word analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from collections import Counter\n",
    "import shelve\n",
    "\n",
    "plt.style.use('seaborn-dark')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Load data we need from previous runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "addresses_df = pd.read_hdf('results/addresses_df_2.h5', 'addresses_df')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "From notebook 2, we already have a single set of unique words across all speeches. Let's fetch that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "with shelve.open('results/vars2') as db:    \n",
    "    all_uq_words = db['all_uq_stemmed_words']\n",
    "    \n",
    "vocab_list = list(all_uq_words)\n",
    "# unique = all_speeches_df.columns.tolist()\n",
    "# unique_set = set(unique)\n",
    "# n_words = len(unique_set)\n",
    "# n_words  # number of unique words across all speeches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now we create a word matrix, whose columns are word vectors for each speech. A word vector contains the word counts for each word across the entire document set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def word_vector(doc, vocab):\n",
    "    \"\"\"Return a word vector for the input document in the context of a given vocabulary.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    doc: iterable of words\n",
    "       \n",
    "    vocab : iterable of words\n",
    "    integer, size of the entire vocabulary across documents.\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    array\n",
    "        An integer array, of length equal to `len(vocab)`, containing the count for each\n",
    "        word in `doc` at its corresponding position in `vocab`.\n",
    "        \n",
    "    Example\n",
    "    -------\n",
    "    \n",
    "    >>> doc = \"b c b c e\".split()\n",
    "    ... vocab = \"a b c d e f\".split()\n",
    "    ... word_vector(doc, vocab)\n",
    "    ... \n",
    "    array([0, 2, 2, 0, 1, 0])\n",
    "    \"\"\"\n",
    "    words_dict = dict.fromkeys(vocab, 0)\n",
    "    doclist = list(doc)\n",
    "    for word in doclist: \n",
    "        words_dict[word] += 1\n",
    "    frequency = list(words_dict.values())\n",
    "    return frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Let's write a simple unit test for this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def test_word_vector():\n",
    "    doc = \"b c b c e\".split()\n",
    "    vocab = \"a b c d e f\".split()\n",
    "    wv = word_vector(doc, vocab)\n",
    "    np.testing.assert_equal(wv, np.array([0, 2, 2, 0, 1, 0]) )\n",
    "\n",
    "test_word_vector()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now let's make the word matrix for our entire set of documents. First, we will go through the dataset and read each speech. In the ingestion of a speech, we'll tokenize and stem each word, removing punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#WARNING: This cell takes a few minutes to run. \n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from string import punctuation \n",
    "import nltk.tokenize\n",
    "\n",
    "def stem_word_tokenize(doc):\n",
    "    \"\"\"custom word tokenizer which removes stop words and punctuation. \n",
    "    Returns the stemmed version of each word. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    doc : string\n",
    "        A document to be tokenized\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    Stemmed tokens.\n",
    "    \"\"\"\n",
    "    \n",
    "    doc = doc.lower() #turn all words to lowercase for standardization \n",
    "    \n",
    "    #From lecture 11\n",
    "    #https://berkeley-stat159-f17.github.io/stat159-f17/lectures/11-strings/11-nltk.html\n",
    "    augmented_punctuation = list(punctuation) + ['--']\n",
    "    empty_content = stopwords.words('english') + augmented_punctuation\n",
    "    \n",
    "    #tokenize words\n",
    "    all_tokens = nltk.tokenize.word_tokenize(doc)\n",
    "    \n",
    "    #return each token if it is not punctuation or a stop word \n",
    "    clean_tokens = [token for token in all_tokens if token not in empty_content]\n",
    "    #stem the words \n",
    "    stemmer = PorterStemmer()\n",
    "    return [stemmer.stem(token) for token in clean_tokens]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with 0 speeches\n",
      "Done with 20 speeches\n",
      "Done with 40 speeches\n",
      "Done with 60 speeches\n",
      "Done with 80 speeches\n",
      "Done with 100 speeches\n",
      "Done with 120 speeches\n",
      "Done with 140 speeches\n",
      "Done with 160 speeches\n",
      "Done with 180 speeches\n",
      "Done with 200 speeches\n",
      "Done with 220 speeches\n"
     ]
    }
   ],
   "source": [
    "speech_matrix = pd.DataFrame(columns=['President', 'Date'] + vocab_list)\n",
    "\n",
    "'''We pass through all of the speeches, creating a word vector for each one.'''\n",
    "with open('data/stateoftheunion1790-2017.txt', 'r') as speeches_txt:\n",
    "    txt_file_chunks = speeches_txt.read().split('***') \n",
    "    \n",
    "    for index, row in addresses_df.iterrows():\n",
    "        raw_speech = txt_file_chunks[index + 1].lower()\n",
    "        stemmed_tokens = stem_word_tokenize(raw_speech)\n",
    "        word_vec = word_vector(stemmed_tokens, all_uq_words)\n",
    "        augmented_vec = [row['President'], row['Date']] + word_vec\n",
    "        for i in range(len(speech_matrix.columns)): \n",
    "            speech_matrix.set_value(index, speech_matrix.columns[i], augmented_vec[i])\n",
    "        if index%20 == 0: \n",
    "            print('Done with {} speeches'.format(index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>217</th>\n",
       "      <th>218</th>\n",
       "      <th>219</th>\n",
       "      <th>220</th>\n",
       "      <th>221</th>\n",
       "      <th>222</th>\n",
       "      <th>223</th>\n",
       "      <th>224</th>\n",
       "      <th>225</th>\n",
       "      <th>226</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ox</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sole</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>puriti</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shower</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cleland</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 227 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0   1   2   3   4   5   6   7   8   9   ... 217 218 219 220 221 222  \\\n",
       "ox        0   0   0   0   0   0   0   0   0   0 ...   0   0   0   0   0   0   \n",
       "sole      0   0   0   0   0   0   0   0   1   0 ...   0   0   0   0   0   0   \n",
       "puriti    0   0   0   0   0   0   0   0   0   0 ...   0   0   0   0   0   0   \n",
       "shower    0   0   0   0   0   0   0   0   0   0 ...   0   0   0   0   0   0   \n",
       "cleland   0   0   0   0   0   0   0   0   0   0 ...   0   0   0   0   0   0   \n",
       "\n",
       "        223 224 225 226  \n",
       "ox        0   0   0   0  \n",
       "sole      0   0   0   0  \n",
       "puriti    0   0   0   0  \n",
       "shower    0   0   0   0  \n",
       "cleland   0   0   0   0  \n",
       "\n",
       "[5 rows x 227 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create a matrix of just the word vector frequencies to do sparsity analysis. \n",
    "wmat = speech_matrix.transpose()\n",
    "wmat = wmat.iloc[2:, :]\n",
    "wmat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "How sparse is this matrix?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wmat is comprised of 93.50% zeros.\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "def sparsity_calculator(text):\n",
    "    rows = len(text)\n",
    "    columns = len(text.columns)\n",
    "    zeros = list((text == 0).sum(axis=1))\n",
    "    total_zeros = sum(zeros)\n",
    "    sparsity = (total_zeros)/(rows*columns)\n",
    "    return sparsity\n",
    "sparsity = sparsity_calculator(wmat)\n",
    "print(f\"wmat is comprised of {100*sparsity:.2f}% zeros.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Intermediate results storage\n",
    "\n",
    "We'll need a few results for the next step, so let's store them in a new set of HDF5/shelve stores for this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import warnings \n",
    "#suppress the PerformanceWarning raised by Pickle when saving such massive files \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "wmat.to_hdf('results/df3.h5', 'wmat')\n",
    "addresses_df.to_hdf('results/df3.addresses_df_3.h5', 'addresses_df')\n",
    "speech_matrix.to_hdf('results/df3.speech_matrix_3.h5', 'speech_matrix')\n",
    "\n",
    "with shelve.open('results/vars3') as db:\n",
    "    db['unique_words'] = all_uq_words\n",
    "    db['vocab_list'] = vocab_list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
